{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":81285,"databundleVersionId":8778365,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport heapq\nfrom tqdm import tqdm\nimport tensorflow as tf\nfrom joblib import Parallel, delayed, cpu_count\n\nimport re\nimport os\nimport heapq\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading the data\ntrain_df = pd.read_csv('/kaggle/input/alx-movie-recommendation-project-2024/train.csv')\ntest_df = pd.read_csv('/kaggle/input/alx-movie-recommendation-project-2024/test.csv')\nsample_submission_df = pd.read_csv('/kaggle/input/alx-movie-recommendation-project-2024/sample_submission.csv')\n\n\nmovies_df = pd.read_csv('/kaggle/input/alx-movie-recommendation-project-2024/movies.csv')\nlinks_df = pd.read_csv('/kaggle/input/alx-movie-recommendation-project-2024/links.csv')\ntags_df = pd.read_csv('/kaggle/input/alx-movie-recommendation-project-2024/tags.csv')\ngenome_scores_df = pd.read_csv('/kaggle/input/alx-movie-recommendation-project-2024/genome_scores.csv')\ngenome_tags_df = pd.read_csv('/kaggle/input/alx-movie-recommendation-project-2024/genome_tags.csv')\nimdb_data_df = pd.read_csv('/kaggle/input/alx-movie-recommendation-project-2024/imdb_data.csv')\n\n# Displaying the first few rows of each DataFrame\ntrain_df.head(), test_df.head(), sample_submission_df.head(), movies_df.head(), links_df.head(), tags_df.head(), genome_scores_df.head(), genome_tags_df.head()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's start by looking at the shape of our datasets\nprint(f\"Train DataFrame shape: {train_df.shape}\")\nprint(f\"Test DataFrame shape: {test_df.shape}\")\nprint(f\"Sample Submission DataFrame shape: {sample_submission_df.shape}\")\n\n# Display basic statistics for the train dataset\ntrain_df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the distribution of ratings\nplt.figure(figsize=(10, 5))\nsns.histplot(train_df['rating'], bins=10, kde=True)\nplt.title(\"Distribution of Movie Ratings\")\nplt.xlabel(\"Rating\")\nplt.ylabel(\"Frequency\")\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the number of ratings per movie\nratings_per_movie = train_df.groupby('movieId').size()\nplt.figure(figsize=(10, 5))\nsns.histplot(ratings_per_movie, bins=50, kde=True)\nplt.title(\"Number of Ratings per Movie\")\nplt.xlabel(\"Number of Ratings\")\nplt.ylabel(\"Frequency\")\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the number of ratings per user\nratings_per_user = train_df.groupby('userId').size()\nplt.figure(figsize=(10, 5))\nsns.histplot(ratings_per_user, bins=50, kde=True)\nplt.title(\"Number of Ratings per User\")\nplt.xlabel(\"Number of Ratings\")\nplt.ylabel(\"Frequency\")\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the average rating per movie\navg_rating_per_movie = train_df.groupby('movieId')['rating'].mean()\nplt.figure(figsize=(10, 5))\nsns.histplot(avg_rating_per_movie, bins=50, kde=True)\nplt.title(\"Average Rating per Movie\")\nplt.xlabel(\"Average Rating\")\nplt.ylabel(\"Frequency\")\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" Data Cleaning and Preprocessing\n","metadata":{}},{"cell_type":"code","source":"# Checking for missing values\nprint(\"Missing values in train_df:\")\nprint(train_df.isnull().sum())\n\nprint(\"Missing values in test_df:\")\nprint(test_df.isnull().sum())\n\nprint(\"Missing values in sample_submission_df:\")\nprint(sample_submission_df.isnull().sum())\n\nprint(\"Missing values in movies_df:\")\nprint(movies_df.isnull().sum())\n\nprint(\"Missing values in links_df:\")\nprint(links_df.isnull().sum())\n\nprint(\"Missing values in tags_df:\")\nprint(tags_df.isnull().sum())\n\nprint(\"Missing values in genome_scores_df:\")\nprint(genome_scores_df.isnull().sum())\n\nprint(\"Missing values in genome_tags_df:\")\nprint(genome_tags_df.isnull().sum())\n\nprint(\"Missing values in imdb_data_df:\")\nprint(imdb_data_df.isnull().sum())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clean text data by removing punctuation and extra spaces\ndef clean_text(text):\n    text = re.sub(r'[^\\w\\s]', '', str(text))\n    text = re.sub('\\s+', ' ', text)\n    return text.strip()\n\n# Clean relevant columns\nmovies_df['title'] = movies_df['title'].apply(clean_text)\nmovies_df['genres'] = movies_df['genres'].apply(clean_text)\nimdb_data_df['title_cast'] = imdb_data_df['title_cast'].apply(clean_text)\nimdb_data_df['director'] = imdb_data_df['director'].apply(clean_text)\nimdb_data_df['plot_keywords'] = imdb_data_df['plot_keywords'].apply(clean_text)\ntags_df['tag'] = tags_df['tag'].apply(clean_text)\ngenome_tags_df['tag'] = genome_tags_df['tag'].apply(clean_text)\n\n# Aggregate tags and genome tags into descriptions\ntags_agg = tags_df.groupby('movieId')['tag'].apply(lambda x: ' '.join(x)).reset_index()\ngenome_tags_agg = genome_scores_df.merge(genome_tags_df, on='tagId')\ngenome_tags_agg = genome_tags_agg.groupby('movieId')['tag'].apply(lambda x: ' '.join(x)).reset_index()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"movies_df.value_counts().unique","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"genome_tags_agg.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize an empty list to store movie descriptions\nmovie_descriptions_list = []\n\n# Iterate through movies_df to append relevant information\nfor index, row in tqdm(movies_df.iterrows(), total=movies_df.shape[0]):\n    movieId = row['movieId']\n    \n    # Get relevant information\n    title = row['title']\n    genres = row['genres']\n    \n    # Find the corresponding row in imdb_data_df\n    imdb_row = imdb_data_df[imdb_data_df['movieId'] == links_df[links_df['movieId'] == movieId]['imdbId'].values[0]]\n    \n    title_cast = imdb_row['title_cast'].values[0] if not imdb_row.empty else ''\n    director = imdb_row['director'].values[0] if not imdb_row.empty else ''\n    plot_keywords = imdb_row['plot_keywords'].values[0] if not imdb_row.empty else ''\n    \n    # Find the corresponding tags and genome tags\n    tag_row = tags_agg[tags_agg['movieId'] == movieId]\n    genome_tag_row = genome_tags_agg[genome_tags_agg['movieId'] == movieId]\n    \n    tags = tag_row['tag'].values[0] if not tag_row.empty else ''\n    genome_tags = genome_tag_row['tag'].values[0] if not genome_tag_row.empty else ''\n    \n    # Combine into a movie description\n    movie_description = f\"{title} {genres} {title_cast} {director} {plot_keywords} {tags} {genome_tags}\"\n    \n    # Append to movie_descriptions_list\n    movie_descriptions_list.append({\n        'movieId': movieId,\n        'movie_description': movie_description\n    })\n\n# Convert the list to a DataFrame\nmovie_descriptions = pd.DataFrame(movie_descriptions_list)\n\n# Fill NaN values in movie_description\nmovie_descriptions['movie_description'].fillna('', inplace=True)\n\n# Display the first few rows of the movie descriptions\nmovie_descriptions.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"movie_descriptions.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Shuffle the movie_descriptions DataFrame\nmovie_descriptions = movie_descriptions.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# Partition the DataFrame into 4 groups\npartitions = np.array_split(movie_descriptions, 4)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tff\n\ntfidf_vectorizers = []\ntfidf_matrices = []\ncosine_sim_matrices = []\n\nfor i, partition in enumerate(partitions):\n    # Initialize TfidfVectorizer for each partition\n    tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), min_df=0.0, stop_words='english')\n    \n    # Fit and transform the movie descriptions\n    tfidf_matrix = tf.fit_transform(partition['movie_description'])\n    \n    # Compute cosine similarity matrix\n    cosine_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n    \n    # Convert cosine similarity matrix to TensorFlow tensor\n    cosine_sim_matrix_tensor = tff.constant(cosine_sim_matrix)\n\n    # Assign to global variables\n    globals()[f'tfidf_{i+1}_matrix'] = tfidf_matrix\n    globals()[f'cosine_sim_{i+1}_matrix'] = cosine_sim_matrix_tensor\n    \n    tfidf_vectorizers.append(tf)\n    tfidf_matrices.append(tfidf_matrix)\n    cosine_sim_matrices.append(cosine_sim_matrix_tensor)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Update indices and titles for partitions\npartition_indices = [pd.Series(partition.index, index=partition['movieId']) for partition in partitions]\n\ndef content_generate_rating_estimate(movie_id, user, rating_data, k=20, threshold=0.0):\n    partition_idx = -1\n    b_idx = -1\n\n    # Find the partition for the movie_id\n    for i, indices in enumerate(partition_indices):\n        if movie_id in indices.index:\n            partition_idx = i + 1\n            b_idx = indices[movie_id]\n            break\n\n    # If the movie_id is not found in any partition, return the average rating\n    if partition_idx == -1:\n        print(f\"Movie ID {movie_id} not found in any partition.\")\n        return rating_data['rating'].mean()\n\n    neighbors = []\n\n    # Iterate through user ratings\n    for index, row in rating_data[rating_data['userId'] == user].iterrows():\n        try:\n            # Find the similarity index\n            sim_idx = partition_indices[partition_idx - 1][row['movieId']]\n            if b_idx < cosine_sim_matrices[partition_idx - 1].shape[0] and sim_idx < cosine_sim_matrices[partition_idx - 1].shape[1]:\n                sim = cosine_sim_matrices[partition_idx - 1][b_idx, sim_idx].numpy()\n                neighbors.append((sim, row['rating']))\n        except (KeyError, IndexError):\n            continue\n\n    # Get the top-k neighbors\n    k_neighbors = heapq.nlargest(k, neighbors, key=lambda t: t[0])\n    simTotal, weightedSum = 0, 0\n\n    # Calculate the predicted rating\n    for (simScore, rating) in k_neighbors:\n        if simScore > threshold:\n            simTotal += simScore\n            weightedSum += simScore * rating\n\n    try:\n        predictedRating = weightedSum / simTotal\n    except ZeroDivisionError:\n        # Fallback to average rating for the movie if no neighbors found\n        predictedRating = np.mean(rating_data[rating_data['movieId'] == movie_id]['rating'])\n\n    return predictedRating\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract userId and movieId from sample_submission_df\nsample_submission_df['userId'] = sample_submission_df['Id'].str.split('_').str[0].astype(int)\nsample_submission_df['movieId'] = sample_submission_df['Id'].str.split('_').str[1].astype(int)\n\n# Function to compute the rating for a single row\ndef compute_rating(row, train_df):\n    return content_generate_rating_estimate(row['movieId'], row['userId'], train_df)\n\n# Function to process a chunk and return the computed ratings\ndef process_chunk(chunk_df, train_df):\n    chunk_df['rating'] = chunk_df.apply(lambda row: compute_rating(row, train_df), axis=1)\n    return chunk_df[['Id', 'rating']]\n\n# Define the chunk size\nchunk_size = 10000  # Adjust chunk size based on memory limits and performance\n\n# Path to the submission file\nsubmission_file_path = 'submission_2.csv'\n\n# Check if the submission file already exists and create if necessary\nif not os.path.exists(submission_file_path):\n    with open(submission_file_path, 'w') as f:\n        f.write('Id,rating\\n')\n\n# Split the sample_submission_df into chunks\nchunks = [sample_submission_df.iloc[i:i + chunk_size] for i in range(0, sample_submission_df.shape[0], chunk_size)]\n\n# Process the chunks in parallel and write the results incrementally\nnum_chunks = len(chunks)\nnum_cores = min(4, cpu_count())  # Adjust number of cores as needed\n\nfor i, chunk in enumerate(tqdm(chunks, desc=\"Processing chunks\")):\n    print(f\"Processing chunk {i+1}/{num_chunks}\")\n    \n    # Compute ratings for the current chunk\n    chunk_ratings = Parallel(n_jobs=num_cores)(delayed(process_chunk)(chunk, train_df) for chunk in [chunk])\n    \n    # Flatten the list of dataframes into a single dataframe\n    result_chunk = pd.concat(chunk_ratings, axis=0)\n    \n    # Append the current chunk to the CSV file\n    result_chunk.to_csv(submission_file_path, mode='a', header=False, index=False)\n\nprint(\"Finished writing to CSV file.\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}